data -> final_test.json files: 
    - Has test cases
    - Prompt + Robots available + Ground truth state of the environment
    IMPROVEMENTS:
    - Can remove hard coded robots as input for each task?
    - give all robots available 
    - Spatial co-ordinates of objects?

data -> pythonic_plans -> train_task_decompose.py 
    - Few shot examples of task decomposition

resources -> actions.py    
    - A string with list of actions separated by commas

resources -> robots.py    
    - List of robots with Skills
    IMPROVEMENT
    - Make list of robots with same skills but initiated at different poses

Prompt IMPROVEMENTS and summary:
- High level task planning: Genrating code for MRS using LLMs.
- One combined long horizon task prompt will take forever to generate dask decomposition. 
- Few shot example -> function call feature makes prompt less long (will save execution time).
- Env: only represented in terms of objects and their mass. No good env representation.
- Multi robot task allocation: Based only robot skills / object mass and mass robot can handle.

#####################################################################################################################3

Chages list from baseline1:
1. number of llms
2. actions parallelization

More baselines from AI2THOR publications:
1. <read more papers>

Novel tasks:
1. Use floor plan/ Bird eye view with VLM for multi-agent action allocation: VIsual room arrangement paper
2. Check if there's a work already done? Not yet, however there are work including complicated scene perception module
3. Maybe having a memory of how task was solved in previous episodes?

3. Long horizon manipulation with coordinated embodied agents 
    - surgical robots operting with different tools
    - sorting a small table top area (active perception)
    - cordinated actions for high level tasks

To Do:
- Replace objects with env scene and see the results
- categorize tasks based on parallelization: Understand all scenes in simulator
- Add Number of robots feature


rgb image from a robot to BEV RGB image(as an image that can be fed to vlm and fine tune on lora):
https://arxiv.org/abs/2008.05711 
https://openaccess.thecvf.com/content/CVPR2023/html/Gosala_SkyEye_Self-Supervised_Birds-Eye-View_Semantic_Mapping_Using_Monocular_Frontal_View_Images_CVPR_2023_paper.html
voxel/pointcloud representation: small area
scene graphs:

#####################################################################################################################

1. ManipulaTHOR implementation: understand fully
2. Read two other papers