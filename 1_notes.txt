SMART LLM CODE:
data -> final_test.json files: 
    - Has test cases
    - Prompt + Robots available + Ground truth state of the environment
    IMPROVEMENTS:
    - Can remove hard coded robots as input for each task?
    - give all robots available 
    - Spatial co-ordinates of objects?

data -> pythonic_plans -> train_task_decompose.py 
    - Few shot examples of task decomposition

resources -> actions.py    
    - A string with list of actions separated by commas

resources -> robots.py    
    - List of robots with Skills
    IMPROVEMENT
    - Make list of robots with same skills but initiated at different poses

Prompt IMPROVEMENTS and summary:
- High level task planning: Genrating code for MRS using LLMs.
- One combined long horizon task prompt will take forever to generate dask decomposition. 
- Few shot example -> function call feature makes prompt less long (will save execution time).
- Env: only represented in terms of objects and their mass. No good env representation.
- Multi robot task allocation: Based only robot skills / object mass and mass robot can handle.

#####################################################################################################################3

CHANGES TO SMART LLM CODE:
1. Reduced number of llms
2. Removed heterogeneous skilled robots: Use case very far from real world robots
3. Added Action parallelization feature
4. Suggests number of robots for efficient implementation of any task

To Do:
- categorize tasks based on parallelization: Understand all scenes in simulator
- Add Number of robots feature

Novel ideas:
1. Use floor plan/ Bird eye view with VLM for multi-agent action allocation: Visual room arrangement paper
3. Maybe having a memory of how task was solved in previous episodes?

Inputs on CV/Scene Representation:
- RGB image from a robot to BEV (as an image that can be fed to vlm and fine tune on lora):
- Papers: 
    - https://arxiv.org/abs/2008.05711 
    - https://openaccess.thecvf.com/content/CVPR2023/html/Gosala_SkyEye_Self-Supervised_Birds-Eye-View_Semantic_Mapping_Using_Monocular_Frontal_View_Images_CVPR_2023_paper.html
- voxel/pointcloud representation: used for small area
- scene graphs: has scene full rep in graph format

#####################################################################################################################
"Coordinated embodied manipulation for complex tasks"

Motivation
    - surgical robots operting with different tools
    - sorting a small table top area (active perception)
    - cordinated actions for high level tasks
Tasks:
    - Read ManipulaTHOR paper
    - Survey paper: https://ieeexplore.ieee.org/abstract/document/8868987  

Novel Ideas:
    - Change urdf file properties of interactable objects to move on floor: e.g., mass of inertia, mass etc.
    - Use meshanything: to create interactable objects based on our requirement for simulator?

Implementation:
    1. Input: Give a scene image input and task desciption
    2. output: 